# -*- coding: utf-8 -*-
"""Ripeness_CNN_Final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14WSNNZ7kXwkd0OCoGo72GnUnalWq5lbI
"""

import os
import random
import numpy as np
import shutil
import tensorflow as tf
from tensorflow.keras import layers, models, optimizers
from tensorflow.keras.preprocessing import image
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications import Xception
from tensorflow.keras.applications.xception import preprocess_input
from tensorflow.keras.optimizers import Adam, Adagrad
from tensorflow.keras.layers import Dense
from tensorflow.keras.models import Model
from tensorflow.keras.utils import plot_model

from sklearn.model_selection import train_test_split

import requests
import tarfile
import PIL
from PIL import Image

from google.colab import drive
drive.mount('/content/gdrive')

import os
import random
import shutil

# Define the path to the main dataset directory containing the class folders
data_dir = '/content/gdrive/MyDrive/full_stack/Banana Ripeness Dataset'

# Define the paths to the train, test, and valid directories
train_dir = 'content/train'
test_dir = 'content/test'
valid_dir = 'content/valid'

# Define the train/test/validation splits
train_split = 0.7
test_split = 0.2
valid_split = 0.1

# Loop through each class folder and create train/test/valid directories for that class
for class_folder in os.listdir(data_dir):
    # Create the train/test/valid directories for this class
    os.makedirs(os.path.join(train_dir, class_folder), exist_ok=True)
    os.makedirs(os.path.join(test_dir, class_folder), exist_ok=True)
    os.makedirs(os.path.join(valid_dir, class_folder), exist_ok=True)

    # Get a list of all files in this class folder
    files = os.listdir(os.path.join(data_dir, class_folder))

    # Shuffle the file list randomly
    random.shuffle(files)

    # Calculate the number of files to use for each split
    num_files = len(files)
    num_train = int(num_files * train_split)
    num_test = int(num_files * test_split)
    num_valid = num_files - num_train - num_test

    # Copy the files into the appropriate train/test/valid directories
    for i, file_name in enumerate(files):
        file_path = os.path.join(data_dir, class_folder, file_name)
        if i < num_train:
            shutil.copy(file_path, os.path.join(train_dir, class_folder, file_name))
        elif i < num_train + num_test:
            shutil.copy(file_path, os.path.join(test_dir, class_folder, file_name))
        else:
            shutil.copy(file_path, os.path.join(valid_dir, class_folder, file_name))

train_dir = "/content/gdrive/MyDrive/full_stack/Banana Ripeness Dataset/train"
val_dir = "/content/gdrive/MyDrive/full_stack/Banana Ripeness Dataset/valid"

BATCH_SIZE = 32
IMG_SIZE = (320, 320)

train_datagen = ImageDataGenerator(
    preprocessing_function=preprocess_input,
    horizontal_flip=True,
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    zoom_range=0.2,
    shear_range=0.2,
    fill_mode="nearest",
)

train_generator = train_datagen.flow_from_directory(
    train_dir,
    target_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    class_mode="categorical",
)

val_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)

val_generator = val_datagen.flow_from_directory(
    val_dir,
    target_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    class_mode="categorical",
)

# Create the base model
base_model = Xception(weights="imagenet", include_top=False, input_shape=IMG_SIZE + (3,))

# Freeze the base model
base_model.trainable = False

# Add a custom classification head
inputs = tf.keras.Input(shape=IMG_SIZE + (3,))
x = base_model(inputs, training=False)
x = layers.GlobalAveragePooling2D()(x)

optimizer_dict = {
    "Adam" : Adam
}

# Define the history dict
history_dict = {}

# Define the number of output layers
num_output_layers = [1, 2]

# Loop over all the combinations of optimizers, learning rates, batch sizes, and number of output layers
for optimizer_name, optimizer in optimizer_dict.items():
    for num_layers in num_output_layers:
        # Add the output layers
        if num_layers == 1:
            output1 = Dense(train_generator.num_classes, activation='softmax')(x)
            outputs = [output1]
        elif num_layers == 2:
            x = Dense(1024, activation='relu')(x)
            output1 = Dense(train_generator.num_classes, activation='softmax')(x)

        # Combine base model and output layers
        model = Model(inputs=inputs, outputs=outputs)

        # Freeze all the layers in the base model
        for layer in base_model.layers:
            layer.trainable = False

        # Compile the model with the current optimizer, learning rate, and batch size
        model.compile(optimizer=optimizer(lr=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])

        # Train the model with the current batch size and output layers
        history = model.fit(train_generator,epochs=20,validation_data=val_generator)

        # Add the model history to the history_dict dictionary with the current hyperparameters as the key
        hyperparameters = (optimizer_name, num_layers)
        history_dict[hyperparameters] = history.history

        # Save the model in Keras format
        model.save('my_model.h5')
        
        # Convert the model to TFLite format
        converter = tf.lite.TFLiteConverter.from_keras_model(model)
        tflite_model = converter.convert()
        
        # Save the TFLite model to a file
        open("my_model.tflite", "wb").write(tflite_model)

import matplotlib.pyplot as plt

# Define the colors for each optimizer
colors = {('Adam', 1): 'blue', ('Adam', 2): 'orange', ('AdaGrad', 1): 'purple', ('AdaGrad', 2): 'red'}

# Define the figure and axes for the plots
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))

# Plot the accuracy and loss for each model in the history_dict
for hyperparameters, history in history_dict.items():
    optimizer_name, num_layers = hyperparameters
    color = colors[(optimizer_name, num_layers)]
    label_train = f"{optimizer_name} ({num_layers} layers) (train)"
    label_val = f"{optimizer_name} ({num_layers} layers) (validation)"
    ax1.plot(history['accuracy'], color=color, label=label_train)
    ax1.plot(history['val_accuracy'], color=color,  linestyle='--', label=label_val)
    ax2.plot(history['loss'], color=color,  label=label_train)
    ax2.plot(history['val_loss'], color=color,  linestyle='--',  label=label_val)

# Set the titles and labels for the plots
ax1.set_title('Model Accuracy')
ax1.set_xlabel('Epoch')
ax1.set_ylabel('Accuracy')
ax1.legend()
ax2.set_title('Model Loss')
ax2.set_xlabel('Epoch')
ax2.set_ylabel('Loss')
ax2.legend()

# Set the x-axis tick frequency to 5
ax1.set_xticks(range(0, len(history['accuracy']), 5))
ax2.set_xticks(range(0, len(history['accuracy']), 5))

# Show the plots
plt.show()

